{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96cc02a",
   "metadata": {},
   "source": [
    "# ë²ˆì—­ê°€ëŠ” ëŒ€í™”ì—ë„ ëŠ¥í•˜ë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460ce9f",
   "metadata": {},
   "source": [
    "ì¢‹ì€ ë²ˆì—­ì„ ë§Œë“œëŠ” ë°ì—ëŠ” ë¬´ìŠ¨ ëŠ¥ë ¥ì´ í•„ìš”í• ê¹Œìš”? \n",
    "\n",
    "ê°€ì¥ ë¨¼ì € ë– ì˜¤ë¥´ëŠ” ê²ƒì€ ì—­ì‹œ ì–¸ì–´ ëŠ¥ë ¥ì´ì£ ! ì ì–´ë„ ë²ˆì—­í•˜ê³ ì í•˜ëŠ” ì–¸ì–´ëŠ” í†µë‹¬í•´ì•¼ ì¢‹ì€ ë²ˆì—­ì„ í•´ë‚¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.  \n",
    "í•˜ì§€ë§Œ ë›°ì–´ë‚œ ì–¸ì–´ ì‹¤ë ¥ë§Œìœ¼ë¡œ ê°€ëŠ¥í• ê¹Œìš”?\n",
    "\n",
    "ë§í•˜ê³  ì‹¶ì€ ê²ƒì€, ë²ˆì—­ê°€ë“¤ì˜ ë²ˆì—­ì´ ë‹¨ìˆœíˆ ì–¸ì–´ë¥¼ ë³€í™˜í•˜ëŠ” ê³¼ì •ì— ê·¸ì¹˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì›ë¬¸ì„ ì´í•´í•˜ê³  ê·¸ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ê¸€ì„ ì‘ë¬¸í•˜ì—¬ íƒ„ìƒí•œë‹¤ëŠ” ê²ë‹ˆë‹¤.\n",
    "\n",
    "ì¸ê³µì§€ëŠ¥ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. \n",
    "\n",
    "ë²ˆì—­ì„ ì˜ í•´ë‚¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì€ ê³§ ì–¸ì–´ë¥¼ ì˜ ì´í•´í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ê¸°ë„ í•´ìš”.  \n",
    "ê·¸ë˜ì„œ ë²ˆì—­ì„ ì˜í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ìì–¸ì–´ ì´í•´(Natural Language Understanding) ëª¨ë¸ì˜ ê·¼ê°„ì´ ë˜ëŠ” ê±°ì£ !.  \n",
    "ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì£¼ê³ ë°›ëŠ” ê²ƒ ë˜í•œ ì œë²• ë†’ì€ ìˆ˜ì¤€ì˜ ìì—°ì–´ ì´í•´ë¥¼ ìš”êµ¬í•˜ëŠ”ë°, ì´ê²ƒë„ ì˜ í•´ë‚¼ ìˆ˜ ìˆì„ì§€...\n",
    "\n",
    "ë²ˆì—­ ëª¨ë¸ì„ í™œìš©í•œ ì±—ë´‡ ë§Œë“¤ê¸°! ì‹œì‘!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18804fb",
   "metadata": {},
   "source": [
    "## ë²ˆì—­ ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "ì´ë²ˆì—” ì ‘ê·¼ì„±ì´ ì¢‹ì€ ì˜ì–´-ìŠ¤í˜ì¸ì–´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c860976",
   "metadata": {},
   "source": [
    "### ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë°ì´í„° ì¤€ë¹„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510271ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f23237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "zip_path = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True\n",
    ")\n",
    "\n",
    "print('ìŠ=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71e03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziipp\\.keras\\datasets/spa-eng/spa.txt\n",
      "Example: 118964\n",
      ">> She went blind.\tElla se quedÃ³ ciega.\n",
      ">> We still have quite a few more miles to go before we get there.\tTodavÃ­a tenemos que recorrer unos cuantos kilÃ³metros mÃ¡s antes de llegar allÃ­.\n",
      ">> If you wish for peace, prepare for war.\tSi querÃ©is paz, preparaos para la guerra.\n",
      ">> Tom couldn't possibly have done what you claimed he did.\tNo es posible que Tom pudiera hacer lo que tÃº alegabas que Ã©l hizo.\n",
      ">> I assumed Tom was waiting for Mary.\tAsumÃ­ que Tom estaba esperando a MarÃ­a.\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.dirname(zip_path)+\"/spa-eng/spa.txt\"\n",
    "print(file_path)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"UTF8\") as f:\n",
    "    spa_eng_sentences = f.read().splitlines()\n",
    "\n",
    "spa_eng_sentences = list(set(spa_eng_sentences)) \n",
    "total_sentence_count = len(spa_eng_sentences)\n",
    "print(\"Example:\", total_sentence_count)\n",
    "\n",
    "for sen in spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbdcccb",
   "metadata": {},
   "source": [
    "#### ê°€ë²¼ìš´ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì„ ì–¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9135395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e4f76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "spa_eng_sentences = list(map(preprocess_sentence, spa_eng_sentences))\n",
    "\n",
    "print('ìŠ=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e87b2",
   "metadata": {},
   "source": [
    "ì´ì œ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ë”°ë¡œ ë–¼ì–´ëƒ…ë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ì˜ 0.5% ì •ë„ë¥¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‚¬ìš©í• ê²Œìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4807bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size:  594\n",
      "\n",
      "\n",
      "Train Example: 118370\n",
      ">> she went blind.\tella se quedÃ³ ciega.\n",
      ">> we still have quite a few more miles to go before we get there.\ttodavÃ­a tenemos que recorrer unos cuantos kilÃ³metros mÃ¡s antes de llegar allÃ­.\n",
      ">> if you wish for peace, prepare for war.\tsi querÃ©is paz, preparaos para la guerra.\n",
      ">> tom couldn't possibly have done what you claimed he did.\tno es posible que tom pudiera hacer lo que tÃº alegabas que Ã©l hizo.\n",
      ">> i assumed tom was waiting for mary.\tasumÃ­ que tom estaba esperando a marÃ­a.\n",
      "\n",
      "\n",
      "Test Example: 594\n",
      ">> tom had no say in the matter.\ttom no tenÃ­a ni voz ni voto en el asunto.\n",
      ">> we found a nail stuck in the tire.\tencontramos un clavo clavado en el neumÃ¡tico.\n",
      ">> you're not ready for this.\tno estÃ¡s listo para esto.\n",
      ">> are you lying to me?\tÂ¿me estÃ¡s mintiendo?\n",
      ">> you've been very quiet.\testÃ¡s muy callada.\n"
     ]
    }
   ],
   "source": [
    "test_sentence_count = total_sentence_count // 200\n",
    "print(\"Test Size: \", test_sentence_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "train_spa_eng_sentences = spa_eng_sentences[:-test_sentence_count]\n",
    "test_spa_eng_sentences = spa_eng_sentences[-test_sentence_count:]\n",
    "print(\"Train Example:\", len(train_spa_eng_sentences))\n",
    "for sen in train_spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "print(\"Test Example:\", len(test_spa_eng_sentences))\n",
    "for sen in test_spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a90313",
   "metadata": {},
   "source": [
    "í•œ ì¤„ì— í¬í•¨ë˜ì–´ ìˆëŠ” ì˜ì–´ì™€ ìŠ¤í˜ì¸ì–´ë¥¼ ë¶„ë¦¬í•´ ì¤ë‹ˆë‹¤.  \n",
    "ì˜ì–´ ë¬¸ì¥ê³¼ ìŠ¤í˜ì¸ì–´ ë¬¸ì¥ì´ tabìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆìœ¼ë‹ˆ split('\\t')ì„ ì‚¬ìš©í•˜ë©´ ë‚˜ëˆŒ ìˆ˜ ìˆê² ë„¤ìš”. tab ì´ì „ì´ ì˜ì–´, ì´í›„ê°€ ìŠ¤í˜ì¸ì–´ ë¬¸ì¥ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fede91bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def split_spa_eng_sentences(spa_eng_sentences):\n",
    "    spa_sentences = []\n",
    "    eng_sentences = []\n",
    "    for spa_eng_sentence in tqdm(spa_eng_sentences):\n",
    "        eng_sentence, spa_sentence = spa_eng_sentence.split('\\t')\n",
    "        spa_sentences.append(spa_sentence)\n",
    "        eng_sentences.append(eng_sentence)\n",
    "    return eng_sentences, spa_sentences\n",
    "\n",
    "print('ìŠ=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4b67b",
   "metadata": {},
   "source": [
    "í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë‘ ë‚˜ëˆ  ì¤ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ab6956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e2e515879f489db4a0b9129035aa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118370\n",
      "she went blind.\n",
      "\n",
      "\n",
      "118370\n",
      "ella se quedÃ³ ciega.\n"
     ]
    }
   ],
   "source": [
    "train_eng_sentences, train_spa_sentences = split_spa_eng_sentences(train_spa_eng_sentences)\n",
    "print(len(train_eng_sentences))\n",
    "print(train_eng_sentences[0])\n",
    "print('\\n')\n",
    "print(len(train_spa_sentences))\n",
    "print(train_spa_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66f0e13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7431ba541c1f4ad2a4c8dcfdd6015c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "tom had no say in the matter.\n",
      "\n",
      "\n",
      "594\n",
      "tom no tenÃ­a ni voz ni voto en el asunto.\n"
     ]
    }
   ],
   "source": [
    "test_eng_sentences, test_spa_sentences = split_spa_eng_sentences(test_spa_eng_sentences)\n",
    "print(len(test_eng_sentences))\n",
    "print(test_eng_sentences[0])\n",
    "print('\\n')\n",
    "print(len(test_spa_sentences))\n",
    "print(test_spa_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf156e",
   "metadata": {},
   "source": [
    "### í† í°í™”\n",
    "\n",
    "ì´ì œ ë¬¸ì¥ ë°ì´í„°ë¥¼ í† í°í™” í•´ì•¼ í•œë‹¤.  \n",
    "<b>Sentencepiece</b> ê¸°ë°˜ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ìƒì„±í•´ ì£¼ëŠ” generate_tokenizer() í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì—¬ í† í¬ë‚˜ì´ì €ë¥¼ ì–»ì–´ë³´ì.  \n",
    "https://github.com/google/sentencepiece ì°¸ê³ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f240fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,   # pad tokenì˜ ì¼ë ¨ë²ˆí˜¸\n",
    "                       bos_id=1,  # ë¬¸ì¥ì˜ ì‹œì‘ì„ ì˜ë¯¸í•˜ëŠ” bos token(<s>)ì˜ ì¼ë ¨ë²ˆí˜¸\n",
    "                       eos_id=2,  # ë¬¸ì¥ì˜ ëì„ ì˜ë¯¸í•˜ëŠ” eos token(</s>)ì˜ ì¼ë ¨ë²ˆí˜¸\n",
    "                       unk_id=3):   # unk tokenì˜ ì¼ë ¨ë²ˆí˜¸\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w', encoding=\"UTF8\") as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30825477",
   "metadata": {},
   "source": [
    "ì´ë²ˆì—” í•œ-ì˜ ë²ˆì—­ ë•Œì™€ ë‹¤ë¥´ê²Œ, ë‘ ì–¸ì–´ê°€ ë‹¨ì–´ ì‚¬ì „ì„ ê³µìœ í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. \n",
    "\n",
    "ì˜ì–´ì™€ ìŠ¤í˜ì¸ì–´ ëª¨ë‘ ì•ŒíŒŒë²³ìœ¼ë¡œ ì´ë¤„ì§€ëŠ” ë°ë‹¤ê°€ ê°™ì€ ì¸ë„ìœ ëŸ½ì–´ì¡±ì´ê¸° ë•Œë¬¸ì— ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ” íš¨ê³¼ê°€ ë§ì•„ìš”! í›„ì— ì±—ë´‡ì„ ë§Œë“¤ ë•Œì—ë„ ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ëª¨ë‘ í•œê¸€ë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— Embedding ì¸µì„ ê³µìœ í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì— ë„ì›€ì´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2106297",
   "metadata": {},
   "source": [
    "ë‹¨ì–´ ì‚¬ì „ ìˆ˜ëŠ” 20,000ìœ¼ë¡œ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤. ì²˜ë¦¬í•˜ëŠ”ë° ì•½ê°„ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49669e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "tokenizer = generate_tokenizer(train_eng_sentences + train_spa_sentences, VOCAB_SIZE, 'spa-eng')\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")  # ë¬¸ì¥ ì–‘ ëì— <s> , </s> ì¶”ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1de97",
   "metadata": {},
   "source": [
    "ìœ„ì—ì„œ ë‘ ì–¸ì–´ ì‚¬ì´ì— ë‹¨ì–´ ì‚¬ì „ì„ ê³µìœ í•˜ê¸°ë¡œ í•˜ì˜€ìœ¼ë¯€ë¡œ Encoderì™€ Decoderì˜ ì „ìš© í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë“¤ì§€ ì•Šê³ ,  \n",
    "ë°©ê¸ˆ ë§Œë“¤ì–´ì§„ í† í¬ë‚˜ì´ì €ë¥¼ ë‘ ì–¸ì–´ ì‚¬ì´ì—ì„œ ê³µìœ í•˜ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864526b1",
   "metadata": {},
   "source": [
    "í† í¬ë‚˜ì´ì €ê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë‹ˆ ë³¸ê²©ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ í† í°í™”í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. \n",
    "\n",
    "í† í°í™”ë¥¼ í•´ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa9da100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def make_corpus(sentences, tokenizer):\n",
    "    corpus = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.encode_as_ids(sentence)\n",
    "        corpus.append(tokens)\n",
    "    return corpus\n",
    "\n",
    "print('ìŠ=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b7b2e",
   "metadata": {},
   "source": [
    "ì˜ì–´ì™€ ìŠ¤í˜ì¸ì–´ë¥¼ ê°ê° í† í°í™” í•´ì¤ë‹ˆë‹¤. í›ˆë ¨ ë°ì´í„°ë§Œ í† í°í™”ë¥¼ í•˜ê³ , ê°™ì€ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì ì— ì£¼ì˜í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1570cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d75b881d7f1430b92054f7026656b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ed1486ae1e4ebb8460817a14b6d38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eng_corpus = make_corpus(train_eng_sentences, tokenizer)\n",
    "spa_corpus = make_corpus(train_spa_sentences, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd475dfa",
   "metadata": {},
   "source": [
    "í† í°í™”ê°€ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "662de5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she went blind.\n",
      "[1, 45, 228, 2180, 0, 2]\n",
      "\n",
      "\n",
      "ella se quedÃ³ ciega.\n",
      "[1, 58, 33, 1069, 6270, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print(train_eng_sentences[0])\n",
    "print(eng_corpus[0])\n",
    "print('\\n')\n",
    "print(train_spa_sentences[0])\n",
    "print(spa_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0e5b5",
   "metadata": {},
   "source": [
    "list ìë£Œí˜•ë„ ë‹¨ìˆ¨ì— íŒ¨ë”© ì‘ì—…ì„ í•´ì£¼ëŠ” ë©‹ì§„ í•¨ìˆ˜ pad_sequences() ë¥¼ ê¸°ì–µí•˜ì‹œì£ ?  \n",
    "ë‹¨ìˆ¨ì— ë°ì´í„°ì…‹ì„ ì™„ì„±í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤! í•œ ë¬¸ì¥ì˜ í† í° ê¸¸ì´ê°€ 50ì´ ë˜ë„ë¡ ì„¤ì •í–ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b3df78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "enc_ndarray = tf.keras.preprocessing.sequence.pad_sequences(eng_corpus, maxlen=MAX_LEN, padding='post')\n",
    "dec_ndarray = tf.keras.preprocessing.sequence.pad_sequences(spa_corpus, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "print('ìŠ=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cba43b",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©ë  ìˆ˜ ìˆë„ë¡ ì˜ì–´ì™€ ìŠ¤í˜ì¸ì–´ ë°ì´í„°ë¥¼ ë¬¶ì–´ ë°°ì¹˜ í¬ê¸°ì˜ í…ì„œë¡œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤. \n",
    "\n",
    "ë°ì´í„° ì…‹ì´ ì™„ì„± ë˜ì—ˆì–´ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b6654e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_ndarray, dec_ndarray)).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "print('ìŠ=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e336809",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì„ ë§Œë“¤ëŸ¬ ê°€ë´…ì‹œë‹¤~!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f48d95",
   "metadata": {},
   "source": [
    "## ë²ˆì—­ ëª¨ë¸ ë§Œë“¤ê¸°\n",
    "\n",
    "### íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬í˜„í•˜ê¸°\n",
    "\n",
    "ìƒì„±ëœ ë°ì´í„°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë©‹ì§„ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!\n",
    "\n",
    "ë‹¨, Encoderì™€ Decoder ê°ê°ì˜ Embeddingê³¼ ì¶œë ¥ì¸µì˜ Linear, ì´ 3ê°œì˜ ë ˆì´ì–´ê°€ Weightë¥¼ ê³µìœ í•  ìˆ˜ ìˆê²Œ í•˜ì„¸ìš”!\n",
    "\n",
    "í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì•„ë˜ì™€ ë™ì¼í•˜ê²Œ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "`transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d02db",
   "metadata": {},
   "source": [
    "### Step 1. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbe35657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding êµ¬í˜„\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e3721",
   "metadata": {},
   "source": [
    "### Step 2. mask ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cedd6d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Mask  ìƒì„±í•˜ê¸°\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7712d3c9",
   "metadata": {},
   "source": [
    "### Step 3. Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1612b777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Multi Head Attention êµ¬í˜„\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, attention_weights\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c32d5",
   "metadata": {},
   "source": [
    "### Step 4. Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85c539b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Position-wise Feed Forward Network êµ¬í˜„\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7312086",
   "metadata": {},
   "source": [
    "### Step 5. Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56f2ad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Encoderì˜ ë ˆì´ì–´ êµ¬í˜„\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e5feb3",
   "metadata": {},
   "source": [
    "### Step 6. Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc938ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder ë ˆì´ì–´ êµ¬í˜„\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V ìˆœì„œì— ì£¼ì˜í•˜ì„¸ìš”!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcf35d",
   "metadata": {},
   "source": [
    "### Step 7. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f8e3da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder êµ¬í˜„\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "            \n",
    "        return out, enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438bf1d",
   "metadata": {},
   "source": [
    "### Step 8. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "059b3bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder êµ¬í˜„\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "            \n",
    "        return out, dec_attns, dec_enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d47e01a",
   "metadata": {},
   "source": [
    "### Step 9. Transformer ì „ì²´ ëª¨ë¸ ì¡°ë¦½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5be9664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5a54f",
   "metadata": {},
   "source": [
    "### Step 10. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7dbeaeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# ì£¼ì–´ì§„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ Transformer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512\n",
    "\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c6d82",
   "metadata": {},
   "source": [
    "ì´ì œ í•™ìŠµì„ ì‹œì¼œë³´ì!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69225c",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f17a189b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate Scheduler êµ¬í˜„\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00746aa5",
   "metadata": {},
   "source": [
    "### Learning Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "891a6c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate ì¸ìŠ¤í„´ìŠ¤ ì„ ì–¸ & Optimizer êµ¬í˜„\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)\n",
    "                                        \n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed36b2f",
   "metadata": {},
   "source": [
    "### Loss Function ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8888b316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Loss Function ì •ì˜\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087c4fc",
   "metadata": {},
   "source": [
    "### Train Step ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7b540dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# Train Step ì •ì˜\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoderì˜ input\n",
    "    gold = tgt[:, 1:]     # Decoderì˜ outputê³¼ ë¹„êµí•˜ê¸° ìœ„í•´ right shiftë¥¼ í†µí•´ ìƒì„±í•œ ìµœì¢… íƒ€ê²Ÿ\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))  \n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a21a3b",
   "metadata": {},
   "source": [
    "í›ˆë ¨ ì‹œí‚¤ì!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffb59eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d376ffd09c64aee92c2bf2e5f05d49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d65be5eb3ec4b588afe6bde27b303bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7299a7b36904db08a8b90dc632aabee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í›ˆë ¨ì‹œí‚¤ê¸°\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_batch,\n",
    "                    dec_batch,\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))\n",
    "        tqdm_bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54827f41",
   "metadata": {},
   "source": [
    "## ë²ˆì—­ ì„±ëŠ¥ ì¸¡ì •í•˜ê¸° (1) BLEU Score\n",
    "\n",
    "https://en.wikipedia.org/wiki/BLEU ì°¸ê³ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28689d",
   "metadata": {},
   "source": [
    "### NLTKë¥¼ í™œìš©í•œ BLEU Score\n",
    "\n",
    "NLTKëŠ” Natural Language Tool Kit ì˜ ì¤€ë§ë¡œ ì´ë¦„ë¶€í„° ìì—°ì–´ ì²˜ë¦¬ì— í° ë„ì›€ì´ ë  ê²ƒ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.ğŸ˜ƒ  \n",
    "nltk ê°€ BLEU Scoreë¥¼ ì§€ì›í•˜ë‹ˆ ì´ë¥¼ í™œìš©í•˜ë„ë¡ í•©ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e36d52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë¬¸: ['ë§', 'ì€', 'ìì—°ì–´', 'ì²˜ë¦¬', 'ì—°êµ¬ì', 'ë“¤', 'ì´', 'íŠ¸ëœìŠ¤í¬ë¨¸', 'ë¥¼', 'ì„ í˜¸', 'í•œë‹¤']\n",
      "ë²ˆì—­ë¬¸: ['ì ', 'ì€', 'ìì—°ì–´', 'í•™', 'ê°œë°œì', 'ë“¤', 'ê°€', 'íŠ¸ëœìŠ¤í¬ë¨¸', 'ì„', 'ì„ í˜¸', 'í•œë‹¤', 'ìš”']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Study\\NotebookProjects\\modoo-nlp\\venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "D:\\Study\\NotebookProjects\\modoo-nlp\\venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# ì•„ë˜ ë‘ ë¬¸ì¥ì„ ë°”ê¿”ê°€ë©° í…ŒìŠ¤íŠ¸ í•´ë³´ì„¸ìš”\n",
    "reference = \"ë§ ì€ ìì—°ì–´ ì²˜ë¦¬ ì—°êµ¬ì ë“¤ ì´ íŠ¸ëœìŠ¤í¬ë¨¸ ë¥¼ ì„ í˜¸ í•œë‹¤\".split()\n",
    "candidate = \"ì  ì€ ìì—°ì–´ í•™ ê°œë°œì ë“¤ ê°€ íŠ¸ëœìŠ¤í¬ë¨¸ ì„ ì„ í˜¸ í•œë‹¤ ìš”\".split()\n",
    "\n",
    "print(\"ì›ë¬¸:\", reference)\n",
    "print(\"ë²ˆì—­ë¬¸:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97f778",
   "metadata": {},
   "source": [
    "BLEU ScoreëŠ” 0~1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ì§€ë§Œ, 100ì„ ê³±í•œ ë°±ë¶„ìœ¨ ê°’ìœ¼ë¡œ í‘œê¸°í•˜ëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. \n",
    "\n",
    "BLEU Scoreì˜ ì ìˆ˜ëŒ€ë³„ í•´ì„ì— ëŒ€í•´ì„œëŠ” https://cloud.google.com/translate/automl/docs/evaluate?hl=ko#bleu ë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cec3c6",
   "metadata": {},
   "source": [
    "BLEU Scoreê°€ 50ì ì„ ë„˜ëŠ”ë‹¤ëŠ” ê²ƒì€ ì •ë§ ë©‹ì§„ ë²ˆì—­ì„ ìƒì„±í–ˆë‹¤ëŠ” ì˜ë¯¸ì˜ˆìš”.  \n",
    "ë³´í†µ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” BLEU ScoreëŠ” 20ì ì—ì„œ ë†’ìœ¼ë©´ 40ì ì„ ë°”ë¼ë³´ëŠ” ì •ë„ê±°ë“ ìš”! í•˜ì§€ë§Œ ë°©ê¸ˆ ë‚˜ì˜¨ ì ìˆ˜ëŠ” ì‚¬ì‹¤ìƒ 0ì ì´ë¼ê³  í•´ì•¼ í•˜ê² ë„¤ìš”. \n",
    "\n",
    "ê·¸ë ‡ê²Œê¹Œì§€ ì—‰ë§ì§„ì°½ì¸ ë²ˆì—­ì´ ëœ ê²ƒì¼ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421b632",
   "metadata": {},
   "source": [
    "BLEU Scoreì˜ ì •ì˜ë¡œ ëŒì•„ê°€ í•œë²ˆ ë”°ì ¸ë´…ì‹œë‹¤. BLEU Scoreê°€ N-gramìœ¼ë¡œ ì ìˆ˜ë¥¼ ì¸¡ì •í•œë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì‹¤ ê±°ì˜ˆìš”. ì•„ë˜ ìˆ˜ì‹ì„ ê¸°ì–µí•˜ì‹œì£ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d17a51da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='capture/17_bleu_score.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539c97d",
   "metadata": {},
   "source": [
    "1-gramë¶€í„° 4-gramê¹Œì§€ì˜ ì ìˆ˜(Precision)ë¥¼ ëª¨ë‘ ê³±í•œ í›„, ë£¨íŠ¸ë¥¼ ë‘ ë²ˆ ì”Œìš°ë©´ BLEU Scoreê°€ ëœë‹µë‹ˆë‹¤. \n",
    "\n",
    "ì§„ì • ë©‹ì§„ ë²ˆì—­ì´ë¼ë©´, ëª¨ë“  N-gramì— ëŒ€í•´ì„œ ë†’ì€ ì ìˆ˜ë¥¼ ì–»ì—ˆì„ ê±°ì˜ˆìš”. \n",
    "\n",
    "ê·¸ë ‡ë‹¤ë©´ ìœ„ì—ì„œ ì‚´í´ë³¸ ì˜ˆì‹œì—ì„œëŠ” ê° N-gramì´ ì ìˆ˜ë¥¼ ì–¼ë§ˆë‚˜ ì–»ì—ˆëŠ”ì§€ í™•ì¸í•´ ë³´ë„ë¡ í•©ì‹œë‹¤.  \n",
    "weightsì˜ ë””í´íŠ¸ ê°’ì€ [0.25, 0.25, 0.25, 0.25]ë¡œ 1-gramë¶€í„° 4-gramê¹Œì§€ì˜ ì ìˆ˜ì— ê°€ì¤‘ì¹˜ë¥¼ ë™ì¼í•˜ê²Œ ì£¼ëŠ” ê²ƒì´ì§€ë§Œ, ë§Œì•½ ì´ ê°’ì„ [1, 0, 0, 0]ìœ¼ë¡œ ë°”ê¿”ì£¼ë©´ BLEU Scoreì— 1-gramì˜ ì ìˆ˜ë§Œ ë°˜ì˜í•˜ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f38f555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd14ea",
   "metadata": {},
   "source": [
    "0ì ì— ê°€ê¹Œìš´ BLEU Scoreê°€ ë‚˜ì˜¤ëŠ” ì›ì¸ì„ ì•Œ ìˆ˜ ìˆê² ë„¤ìš”.  \n",
    "ë°”ë¡œ 3-gramì™€ 4-gramì—ì„œ ê±°ì˜ 0ì ì„ ë°›ì•˜ê¸° ë•Œë¬¸ì¸ë°ìš”, ìœ„ ì˜ˆì‹œì—ì„œ ë²ˆì—­ë¬¸ ë¬¸ì¥ ì¤‘ ì–´ëŠ 3-gramë„ ì›ë¬¸ì˜ 3-gramê³¼ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 2-gramì´ 0.18ì´ ë‚˜ì˜¤ëŠ” ê²ƒì€ ì›ë¬¸ì˜ 11ê°œ 2-gram ì¤‘ì— 2ê°œë§Œì´ ë²ˆì—­ë¬¸ì—ì„œ ì¬í˜„ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b890782f",
   "metadata": {},
   "source": [
    "### SmoothingFunction()ìœ¼ë¡œ BLEU Score ë³´ì •í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed091e6f",
   "metadata": {},
   "source": [
    "ê·¸ë˜ì„œ BLEU ê³„ì‚°ì‹œ íŠ¹ì • N-gramì´ 0ì ì´ ë‚˜ì™€ì„œ BLEUê°€ ë„ˆë¬´ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ìª½ìœ¼ë¡œ ì™œê³¡ë˜ëŠ” ë¬¸ì œë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ SmoothingFunction() ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "Smoothing í•¨ìˆ˜ëŠ” ëª¨ë“  Precisionì— ì•„ì£¼ ì‘ì€ epsilon ê°’ì„ ë”í•´ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ”ë°, ì´ë¡œì¨ 0ì ì´ ë¶€ì—¬ëœ Precisionë„ ì™„ì „í•œ 0ì´ ë˜ì§€ ì•Šìœ¼ë‹ˆ ì ìˆ˜ë¥¼ 1.0 ìœ¼ë¡œ ëŒ€ì²´í•  í•„ìš”ê°€ ì—†ì–´ì§€ì£ . ì¦‰ ìš°ë¦¬ì˜ ì˜ë„ëŒ€ë¡œ ì ìˆ˜ê°€ ê³„ì‚°ë˜ëŠ” ê±°ì˜ˆìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7159792a",
   "metadata": {},
   "source": [
    "ì§„ì‹¤ëœ BLEU Scoreë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì–´ì„œ SmoothingFunction() ì„ ì ìš©í•´ ë´…ì‹œë‹¤! \n",
    "\n",
    "ì•„ë˜ ì½”ë“œì—ì„œëŠ” SmoothingFunction().method1ì„ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤.  \n",
    "ìì‹ ë§Œì˜ Smoothing í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ì„œ ì ìš©í•  ìˆ˜ë„ ìˆê² ì§€ë§Œ, nltkì—ì„œëŠ” method0ë¶€í„° method7ê¹Œì§€ë¥¼ ì´ë¯¸ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c10cb624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function ì ìš©\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e59d0e",
   "metadata": {},
   "source": [
    "SmoothingFunction()ë¡œ BLEU scoreë¥¼ ë³´ì •í•œ ê²°ê³¼, ìƒˆë¡œìš´ BLEU ì ìˆ˜ëŠ” ë¬´ë ¤, 5ì ìœ¼ë¡œ ì˜¬ë¼ê°”ìŠµë‹ˆë‹¤. \n",
    "\n",
    "ê±°ì˜ ì˜ë¯¸ ì—†ëŠ” ë²ˆì—­ì´ë¼ëŠ” ëƒ‰ì •í•œ í‰ê°€ë¥¼ ë°›ê²Œ ë˜ëŠ”êµ°ìš”.ğŸ˜¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c77f1",
   "metadata": {},
   "source": [
    "ì—¬ê¸°ì„œ BLEU-4ê°€ BLEU-3ë³´ë‹¤ ì¡°ê¸ˆì´ë‚˜ë§ˆ ì ìˆ˜ê°€ ë†’ì€ ì´ìœ ëŠ” í•œ ë¬¸ì¥ì—ì„œ ë°œìƒí•˜ëŠ” 3-gram ìŒì˜ ê°œìˆ˜ì™€ 4-gram ìŒì˜ ê°œìˆ˜ë¥¼ ìƒê°í•´ ë³´ë©´ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° Precisionì„ N-gram ê°œìˆ˜ë¡œ ë‚˜ëˆ„ëŠ” ë¶€ë¶„ì—ì„œ ì°¨ì´ê°€ ë°œìƒí•˜ëŠ” ê²ƒì´ì£ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b8b1a",
   "metadata": {},
   "source": [
    "### íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ë²ˆì—­ ì„±ëŠ¥ ì•Œì•„ë³´ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d9cef",
   "metadata": {},
   "source": [
    "ìœ„ ì˜ˆì‹œë¥¼ ì¡°ê¸ˆë§Œ ì‘ìš©í•˜ë©´ ìš°ë¦¬ê°€ í›ˆë ¨í•œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë²ˆì—­ì„ ì˜í•˜ëŠ”ì§€ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!.  \n",
    "ì•„ê¹Œ 0.5%ì˜ ë°ì´í„°ë¥¼ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ë¹¼ ë‘” ê²ƒì„ ê¸°ì–µí•˜ì‹œì£ ? í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ëª¨ë¸ì˜ BLEU Scoreë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ eval_bleu() ë¥¼ êµ¬í˜„í•´ë³´ë„ë¡ í•©ì‹œë‹¤!\n",
    "\n",
    "ë¨¼ì € ë²ˆì—­ê¸°ê°€ ë¬¸ì¥ì„ ìƒì„±í•˜ë„ë¡ translate() í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b3e4b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def translate(tokens, model, src_tokenizer, tgt_tokenizer):\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=MAX_LEN,\n",
    "                                                           padding='post')\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)   \n",
    "    for i in range(MAX_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(padded_tokens, output)\n",
    "\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                      output,\n",
    "                                      enc_padding_mask,\n",
    "                                      combined_mask,\n",
    "                                      dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)  \n",
    "            return result\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)  \n",
    "    return result\n",
    "\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b0a5b4",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ ë²ˆì—­í•œ ë¬¸ì¥ì˜ BLEU Scoreë¥¼ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "ìš°ì„  í•œ ë¬¸ì¥ë§Œ í‰ê°€í•˜ëŠ” eval_bleu_singleì„ ë§Œë“¤ì–´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7798a279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    src_tokens = src_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_tokens = tgt_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    if (len(src_tokens) > MAX_LEN): return None\n",
    "    if (len(tgt_tokens) > MAX_LEN): return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = translate(src_tokens, model, src_tokenizer, tgt_tokenizer).split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score\n",
    "        \n",
    "print('ìŠ=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b37ffc",
   "metadata": {},
   "source": [
    "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤‘ì— í•˜ë‚˜ë¥¼ ê³¨ë¼ í‰ê°€í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d219f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  tom fixed everything.\n",
      "Model Prediction:  ['tom', 'arreglÃ³', 'todo', 'el', 'mundo', 'lo', 'que', 'le', 'dÃ©ndes', 'todo', 'el', 'mundo', 'lo', 'que', 'tom', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo', 'demÃ¡s', 'lo']\n",
      "Real:  ['tom', 'arreglÃ³', 'todo.']\n",
      "Score: 0.008453\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008453462482287115"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì¸ë±ìŠ¤ë¥¼ ë°”ê¿”ê°€ë©° í…ŒìŠ¤íŠ¸í•´ ë³´ì„¸ìš”\n",
    "test_idx = 1\n",
    "\n",
    "eval_bleu_single(transformer, \n",
    "                 test_eng_sentences[test_idx], \n",
    "                 test_spa_sentences[test_idx], \n",
    "                 tokenizer, \n",
    "                 tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c71d6",
   "metadata": {},
   "source": [
    "ì´ì œ ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œ í‰ê°€í•´ ë´…ì‹œë‹¤. eval_bleu_singleì„ ì´ìš©í•´ì„œ eval_bleu í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc000b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)\n",
    "    \n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af1128",
   "metadata": {},
   "source": [
    "í‰ê°€í•´ ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52b8df40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c9e118c20c4325b54731e6edf29708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 594\n",
      "Total Score: 0.09803853530102441\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, test_eng_sentences, test_spa_sentences, tokenizer, tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a97baf",
   "metadata": {},
   "source": [
    "## ë²ˆì—­ ì„±ëŠ¥ ì¸¡ì •í•˜ê¸° (2) Beam Search Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805b7b7",
   "metadata": {},
   "source": [
    "ì´ ë©‹ì§„ í‰ê°€ ì§€í‘œë¥¼ ë” ë©‹ì§€ê²Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•! ë°”ë¡œ ëª¨ë¸ì˜ ìƒì„± ê¸°ë²•ì— ë³€í™”ë¥¼ ì£¼ëŠ” ê²ƒì´ì£ . Greedy Decoding ëŒ€ì‹  ìƒˆë¡œìš´ ê¸°ë²•ì„ ì ìš©í•˜ë©´ ìš°ë¦¬ ëª¨ë¸ì„ ë” ì˜ í‰ê°€í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ë„¤ìš”!\n",
    "\n",
    "Beam Searchë¥¼ ê¸°ì–µí•˜ë‚˜ìš”? ì˜ˆì‹œë¡œ í™œìš©í–ˆë˜ ì½”ë“œë¥¼ ë‹¤ì‹œ í•œë²ˆ ì‚´í´ë³´ë©´,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23748fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]]  # ìƒì„±ëœ ë¬¸ì¥ê³¼ ì ìˆ˜ë¥¼ ì €ì¥\n",
    "\n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # ê° ë‹¨ì–´ì˜ í™•ë¥ ì„ ì´ì ì— ëˆ„ì  ê³±\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates,\n",
    "                         key=lambda tup:tup[1],\n",
    "                         reverse=True) # ì´ì  ìˆœ ì •ë ¬\n",
    "        sequences = ordered[:beam_size] # Beam Sizeì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ë§Œ ì €ì¥ \n",
    "\n",
    "    return sequences\n",
    "\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e869119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì»¤í”¼ ë¥¼ ê°€ì ¸ ë„ ë  ê¹Œìš”? <pad> <pad> <pad> <pad>  // Score: 42.5243\n",
      "ì»¤í”¼ ë¥¼ ë§ˆì…” ë„ ë  ê¹Œìš”? <pad> <pad> <pad> <pad>  // Score: 28.0135\n",
      "ë§ˆì…” ë¥¼ ê°€ì ¸ ë„ ë  ê¹Œìš”? <pad> <pad> <pad> <pad>  // Score: 17.8983\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"ê¹Œìš”?\",\n",
    "    2: \"ì»¤í”¼\",\n",
    "    3: \"ë§ˆì…”\",\n",
    "    4: \"ê°€ì ¸\",\n",
    "    5: \"ë \",\n",
    "    6: \"ë¥¼\",\n",
    "    7: \"í•œ\",\n",
    "    8: \"ì”\",\n",
    "    9: \"ë„\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in seq:\n",
    "        sentence += vocab[word] + \" \"\n",
    "\n",
    "    print(sentence, \"// Score: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b0b47",
   "metadata": {},
   "source": [
    "ì‚¬ì‹¤ ì´ ì˜ˆì‹œëŠ” Beam Searchë¥¼ ì„¤ëª…í•˜ëŠ” ë°ì—ëŠ” ë”ì—†ì´ ì ë‹¹í•˜ì§€ë§Œ ì‹¤ì œë¡œ ëª¨ë¸ì´ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ê³¼ëŠ” ê±°ë¦¬ê°€ ë©‰ë‹ˆë‹¤.  \n",
    "ë‹¹ì¥ ëª¨ë¸ì´ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ë§Œ ë– ì˜¬ë ¤ë„ ìœ„ì˜ prob_seq ì²˜ëŸ¼ í™•ë¥ ì„ ì •ì˜í•  ìˆ˜ ì—†ê² ë‹¤ëŠ” ìƒê°ì´ ë¨¸ë¦¬ë¥¼ ìŠ¤ì¹˜ì£ .  \n",
    "ê° ë‹¨ì–´ì— ëŒ€í•œ í™•ë¥ ì€ prob_seq ì²˜ëŸ¼ í•œ ë²ˆì— ì •ì˜ê°€ ë˜ì§€ ì•Šê³  ì´ì „ ìŠ¤í…ê¹Œì§€ì˜ ë‹¨ì–´ì— ë”°ë¼ì„œ ê²°ì •ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤!\n",
    "\n",
    "ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ, Beam Sizeê°€ 2ì´ê³  Time-stepì´ 2ì¸ ìˆœê°„ì˜ ë‘ ë¬¸ì¥ì´ ë‚˜ëŠ” ë°¥ì„ , ë‚˜ëŠ” ì»¤í”¼ë¥¼ ì´ë¼ê³  í•œë‹¤ë©´ ì„¸ ë²ˆì§¸ ë‹¨ì–´ë¡œ ë¨¹ëŠ”ë‹¤ , ë§ˆì‹ ë‹¤ ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ, ì „ìì—ì„œ ë§ˆì‹ ë‹¤ ì— í• ë‹¹í•˜ëŠ” í™•ë¥ ê³¼ í›„ìì—ì„œ ë§ˆì‹ ë‹¤ ì— í• ë‹¹í•˜ëŠ” í™•ë¥ ì€ ê°ê° ì´ì „ ë‹¨ì–´ë“¤ì¸ ë‚˜ëŠ” ë°¥ì„ , ë‚˜ëŠ” ì»¤í”¼ë¥¼ ì— ë”°ë¼ì„œ ê²°ì •ë˜ê¸° ë•Œë¬¸ì— ì„œë¡œ ë…ë¦½ì ì¸ í™•ë¥ ì„ ê°–ìŠµë‹ˆë‹¤. ì˜ˆì»¨ëŒ€ í›„ìê°€ ë§ˆì‹ ë‹¤ ì— ë” ë†’ì€ í™•ë¥ ì„ í• ë‹¹í•  ê²ƒì„ ì•Œ ìˆ˜ ìˆì£ ! ìœ„ ì†ŒìŠ¤ì—ì„œì²˜ëŸ¼ \"3ë²ˆì§¸ ë‹¨ì–´ëŠ” í•­ìƒ [ë§ˆì‹ ë‹¤: 0.3, ë¨¹ëŠ”ë‹¤:0.5, ...] ì˜ í™•ë¥ ì„ ê°€ì§„ë‹¤!\" ë¼ê³ ëŠ” í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ë‹ˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ Beam Searchë¥¼ ìƒì„± ê¸°ë²•ìœ¼ë¡œ êµ¬í˜„í•  ë•Œì—ëŠ” ë¶„ê¸°ë¥¼ ì˜ ë‚˜ëˆ ì¤˜ì•¼ í•©ë‹ˆë‹¤.  \n",
    "Beam Sizeê°€ 5ë¼ê³  ê°€ì •í•˜ë©´ ë§¨ ì²« ë‹¨ì–´ë¡œ ì í•©í•œ 5ê°œì˜ ë‹¨ì–´ë¥¼ ìƒì„±í•˜ê³ , ë‘ ë²ˆì§¸ ë‹¨ì–´ë¡œ ê° ì²« ë‹¨ì–´(5ê°œ ë‹¨ì–´)ì— ëŒ€í•´ 5ìˆœìœ„ê¹Œì§€ í™•ë¥ ì„ êµ¬í•˜ì—¬ ì´ 25ê°œì˜ ë¬¸ì¥ì„ ìƒì„±í•˜ì£ . ê·¸ 25ê°œì˜ ë¬¸ì¥ë“¤ì€ ê° ë‹¨ì–´ì— í• ë‹¹ëœ í™•ë¥ ì„ ê³±í•˜ì—¬ êµ¬í•œ ì ìˆ˜(ì¡´ì¬ í™•ë¥ ) ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë‹ˆ ê°ê°ì˜ ìˆœìœ„ë¥¼ ë§¤ê¸¸ ìˆ˜ ìˆê² ì£ ? ì ìˆ˜ ìƒìœ„ 5ê°œì˜ í‘œë³¸ë§Œ ì‚´ì•„ë‚¨ì•„ ì„¸ ë²ˆì§¸ ë‹¨ì–´ë¥¼ êµ¬í•  ìê²©ì„ ì–»ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ìœ„ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ìµœì¢…ì ìœ¼ë¡œ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ 5ê°œì˜ ë¬¸ì¥ì„ ì–»ê²Œ ë©ë‹ˆë‹¤. ë¬¼ë¡  Beam Sizeë¥¼ ì¡°ì ˆí•´ ì£¼ë©´ ê·¸ ìˆ˜ëŠ” ìœ ë™ì ìœ¼ë¡œ ë³€í•  ê±°êµ¬ìš”! \n",
    "\n",
    "ë‹¤ë“¤ ì˜ ì´í•´í•˜ì…¨ì£ ? ğŸ˜ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb141849",
   "metadata": {},
   "source": [
    "### Beam Search Decoder ì‘ì„± ë° í‰ê°€í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433037e7",
   "metadata": {},
   "source": [
    "ê° ë‹¨ì–´ì˜ í™•ë¥ ê°’ì„ ê³„ì‚°í•˜ëŠ” calc_prob()ì™€ Beam Searchë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ëŠ” beam_search_decoder() ë¥¼ êµ¬í˜„í•˜ê³  ìƒì„±ëœ ë¬¸ì¥ì— ëŒ€í•´ BLEU Scoreë¥¼ ì¶œë ¥í•˜ëŠ” beam_bleu() ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!\n",
    "\n",
    "í¸ì˜ì— ë”°ë¼ì„œ ë‘ ê¸°ëŠ¥ì„ í•˜ë‚˜ì˜ í•¨ìˆ˜ì— êµ¬í˜„í•´ë„ ì¢‹ìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d25790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# calc_prob() êµ¬í˜„\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "    model(src_ids, \n",
    "            tgt_ids,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask)\n",
    "            \n",
    "    return tf.math.softmax(predictions, axis=-1)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "730b397c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# beam_search_decoder() êµ¬í˜„\n",
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "    \n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.int64)\n",
    "    pred_tmp = np.zeros((beam_size, tgt_len), dtype=np.int64)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.int64)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred_tmp[:, 0] = tgt_tokenizer.bos_id()\n",
    "\n",
    "    dec_in = tf.expand_dims(pred_tmp[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred_tmp[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # ëª¨ë“  Branchë¥¼ ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ë¥¼ ë°©ì§€\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred_tmp[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.eos_id():\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    pred = []\n",
    "    for long_pred in pred_tmp:\n",
    "        zero_idx = long_pred.tolist().index(tgt_tokenizer.eos_id())\n",
    "        short_pred = long_pred[:zero_idx+1]\n",
    "        pred.append(short_pred)\n",
    "    return pred\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c8706",
   "metadata": {},
   "source": [
    "BLEU ê³„ì‚° í•¨ìˆ˜ëŠ” ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©í• ê²Œìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b0587a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "print('ìŠ=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45719f52",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ beam_bleuí•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0614222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# beam_bleu() êµ¬í˜„\n",
    "def beam_bleu(reference, ids, tokenizer):\n",
    "    reference = reference.split()\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        candidate = tokenizer.decode_ids(_id.tolist()).split()\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "\n",
    "        total_score += score\n",
    "    return total_score / len(ids)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d41a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: ['la', 'herida', 'se', 'infectÃ³.']\n",
      "Candidate: ['la', 'herida', 'se', 'volviÃ³', 'profesora', 'a', 'tiempo']\n",
      "BLEU: 0.16348126556655487\n",
      "Reference: ['la', 'herida', 'se', 'infectÃ³.']\n",
      "Candidate: ['la', 'herida', 'se', 'volviÃ³', 'contra', 'a', 'tiempo']\n",
      "BLEU: 0.16348126556655487\n",
      "Reference: ['la', 'herida', 'se', 'infectÃ³.']\n",
      "Candidate: ['la', 'herida', 'se', 'volviÃ³', 'profesora', 'a', 'fondo']\n",
      "BLEU: 0.16348126556655487\n",
      "Reference: ['la', 'herida', 'se', 'infectÃ³.']\n",
      "Candidate: ['la', 'herida', 'se', 'volviÃ³', 'profesora', 'a', 'la']\n",
      "BLEU: 0.16348126556655487\n",
      "Reference: ['la', 'herida', 'se', 'infectÃ³.']\n",
      "Candidate: ['la', 'herida', 'se', 'volviÃ³', 'paradero', 'a', 'tiempo']\n",
      "BLEU: 0.16348126556655487\n",
      "0.16348126556655487\n"
     ]
    }
   ],
   "source": [
    "# ì¸ë±ìŠ¤ë¥¼ ë°”ê¿”ê°€ë©° í™•ì¸í•´ ë³´ì„¸ìš”\n",
    "test_idx = 2\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(test_eng_sentences[test_idx],\n",
    "                    MAX_LEN,\n",
    "                    MAX_LEN,\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(test_spa_sentences[test_idx], ids, tokenizer)\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91844ec3",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ë¶€í’€ë¦¬ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aead95",
   "metadata": {},
   "source": [
    "ì´ë²ˆ ìŠ¤í…ì—ì„œëŠ” Data Augmentation, ê·¸ì¤‘ì—ì„œë„ Embeddingì„ í™œìš©í•œ Lexical Substitutionì„ êµ¬í˜„í•´ ë³¼ ê±°ì˜ˆìš”.  \n",
    "gensim ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ë©´ ì–´ë µì§€ ì•Šê²Œ í•´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "gensim ì— ì‚¬ì „ í›ˆë ¨ëœ Embedding ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì€ ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- ì§ì ‘ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ load í•˜ëŠ” ë°©ë²•\n",
    "- gensim ì´ ìì²´ì ìœ¼ë¡œ ì§€ì›í•˜ëŠ” downloader ë¥¼ í™œìš©í•´ ëª¨ë¸ì„ load í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "í•œêµ­ì–´ëŠ” gensim ì—ì„œ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë‘ ë²ˆì§¸ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì§€ë§Œ, ì˜ì–´ë¼ë©´ ì–˜ê¸°ê°€ ë‹¬ë¼ì§€ì£ !.  \n",
    "ì•„ë˜ ì›¹í˜ì´ì§€ì˜ Available data â†’ Model ë¶€ë¶„ì—ì„œ ê³µê°œëœ ëª¨ë¸ì˜ ì¢…ë¥˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "https://github.com/RaRe-Technologies/gensim-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b072c",
   "metadata": {},
   "source": [
    "ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” Embedding ëª¨ë¸ì€ word2vec-google-news-300 ì´ì§€ë§Œ ìš©ëŸ‰ì´ ì»¤ì„œ ë‹¤ìš´ë¡œë“œì— ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ë¯€ë¡œ ì´ë²ˆ ì‹¤ìŠµì—” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n",
    "\n",
    "ìš°ë¦¬ëŠ” ì ë‹¹í•œ ì‚¬ì´ì¦ˆì˜ ëª¨ë¸ì¸ glove-wiki-gigaword-300 ì„ ì‚¬ìš©í• ê²Œìš”! ì•„ë˜ ì†ŒìŠ¤ë¥¼ ì‹¤í–‰í•´ ì‚¬ì „ í›ˆë ¨ëœ Embedding ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bd1b5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37e7c4",
   "metadata": {},
   "source": [
    "ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ì´ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "442328d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bananas', 0.6691170930862427),\n",
       " ('mango', 0.5804104208946228),\n",
       " ('pineapple', 0.5492372512817383),\n",
       " ('coconut', 0.5462778806686401),\n",
       " ('papaya', 0.541056752204895),\n",
       " ('fruit', 0.52181077003479),\n",
       " ('growers', 0.4877638816833496),\n",
       " ('nut', 0.48399588465690613),\n",
       " ('peanut', 0.48062023520469666),\n",
       " ('potato', 0.48061180114746094)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a352f",
   "metadata": {},
   "source": [
    "ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œ í›„, ëœë¤í•˜ê²Œ í•˜ë‚˜ë¥¼ ì„ ì •í•˜ì—¬ í•´ë‹¹ í† í°ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ì°¾ì•„ ëŒ€ì¹˜í•˜ë©´ ê·¸ê²ƒìœ¼ë¡œ Lexical Substitutionì€ ì™„ì„±ë˜ê² ì£ ? \n",
    "\n",
    "ê°€ë³ê²Œ í™•ì¸í•´ ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b6f2ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: you know ? all you need is attention .\n",
      "To: you know ? all you need is focus . \n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"you know ? all you need is attention .\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += wv.most_similar(tok)[0][0] + \" \"\n",
    "\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195f372",
   "metadata": {},
   "source": [
    "### Lexical Substitution êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46373e9f",
   "metadata": {},
   "source": [
    "ì…ë ¥ëœ ë¬¸ì¥ì„ Embedding ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Augmentation í•˜ì—¬ ë°˜í™˜í•˜ëŠ” lexical_sub() ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdb081fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Substitution êµ¬í˜„í•˜ê¸°\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    res = \"\"\n",
    "    toks = sentence.split()\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "        \n",
    "    except:   # ë‹¨ì–´ì¥ì— ì—†ëŠ” ë‹¨ì–´\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97803dab",
   "metadata": {},
   "source": [
    "ë§Œë“¤ì–´ì§„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë³¼ê¹Œìš”? ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë‹ˆ ìš°ì„  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ Augmentationë“¤ì„ ë§Œë“¤ì–´ ë´…ì‹œë‹¤. ì•½ê°„ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤. \n",
    "\n",
    "í”„ë¡œì íŠ¸ì—ì„œëŠ” í•™ìŠµ ë°ì´í„°ë¡œ Augmentationì„ í•´ì•¼í•©ë‹ˆë‹¤~!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32115f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691956634fad4d1aab3ec2479a0425f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom had no say in the matter.', 'tom rate everything. ', 'tom fixed everything.', 'the wound became infected.', \"don't you you the clock? \", \"don't you see the clock?\", 'someone broke of lock and stole my bike. ', 'someone broke the lock and stole my bike.', 'are they come here tomorrow? ', 'are they coming here tomorrow?']\n"
     ]
    }
   ],
   "source": [
    "new_corpus = []\n",
    "\n",
    "for old_src in tqdm(test_eng_sentences):\n",
    "    new_src = lexical_sub(old_src, wv)\n",
    "    if new_src is not None: \n",
    "        new_corpus.append(new_src)\n",
    "    # Augmentationì´ ì—†ë”ë¼ë„ ì›ë³¸ ë¬¸ì¥ì„ í¬í•¨ì‹œí‚µë‹ˆë‹¤\n",
    "    new_corpus.append(old_src)\n",
    "\n",
    "print(new_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944ada1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
