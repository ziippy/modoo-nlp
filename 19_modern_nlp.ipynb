{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc9fb25",
   "metadata": {},
   "source": [
    "# modern NLP 의 흐름에 올라타보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f63b2db",
   "metadata": {},
   "source": [
    "## 변화의 흐름 - Word Embedding 과 Context\n",
    "\n",
    "우리는 NLP task 를 풀기 위하여 인간의 언어를 컴퓨터의 언어로 바꾸어 주는 작업을 반드시 거쳐야만 합니다. (임베딩)  \n",
    "단어가 가지고 있는 의미를 숫자에 녹여내기 위해서 tf-idf, word2vec, fasttext 등 다양한 임베딩 방법들을 공부해 보기도 했지요.  \n",
    "우리가 기존에 배웠던 임베딩 방식은 워드임베딩(Word Embedding)이라고 합니다. 즉, 단어 하나하나를 임베딩하는 방식이지요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e3b03",
   "metadata": {},
   "source": [
    "그런데..\n",
    "\n",
    "생긴 것은 똑같이 생겼는데 다른 뜻을 가지고 있는 단어들(동음이의어, 다의어 등)은 어떻게 표현될지? 생각해 본 적 있나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f804e1",
   "metadata": {},
   "source": [
    "예시: 차\n",
    "- 시간이 늦었으니 커피 말고 차나 한 잔 마시자\n",
    "- 제주도는 버스로 움직이기 힘들어. 차가 있어야 해.\n",
    "- 축구공을 이쪽으로 차.\n",
    "\n",
    "똑똑한 여러분은 '문맥을 고려하는 임베딩(Contextual Embedding)을 만들면 되지 않나?!'라고 생각하실 겁니다.\n",
    "\n",
    "바로 앞으로 등장하는 모델들은 이러한 문맥(context) 을 잘 반영하는 모델들입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24e1b0",
   "metadata": {},
   "source": [
    "## Transfer Learning과 Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a1565",
   "metadata": {},
   "source": [
    "### Transfer Learning (전이 학습)\n",
    "\n",
    "자연어에서의 전이 학습은 언제, 어디서 사용되는 것일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f8d87b",
   "metadata": {},
   "source": [
    "### Language Modeling(언어 모델)\n",
    "\n",
    "자연어 처리에서의 전이 학습은 보통 language model(언어 모델)과 관련이 깊습니다.\n",
    "\n",
    "언어 모델은 \n",
    "- 입력으로 주어진 시퀀스의 다음을 학습하는 과정에서 주어진 시퀀스가 얼마나 자연스러운지를 학습하게 됩니다. \n",
    "- 즉, 언어 모델은 철수가 밥을 마셨다인지 철수가 밥을 먹었다인지를 데이터로부터 학습을 하게 됩니다. 이렇게 학습을 완료한 언 어모델은 언어의 패턴과 규칙을 학습하여 전반적인 언어의 특징을 익히게 됩니다.\n",
    "\n",
    "자연어 처리에서 바로 이 언어 모델이 pretrained model이 되는 것입니다. \n",
    "- 이미 언어의 전반적인 것을 아는 신경망에게 언어와 관련된 문제를 풀게 하는 것이지요. \n",
    "- 이처럼 주어진 문제(다운스트림 테스크 혹은 downstream task)를 잘 풀기 위해 pretrained model을 재학습시키는 것을 fine-tuning이라고 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0360f",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "사실 최근 NLP의 가장 큰 흐름은 트랜스포머라고 해도 무방하다고 생각합니다. \n",
    "\n",
    "GPT, BERT 등 이후에 만들어지는 모델들은 트랜스포머가 기반이기 때문이죠.\n",
    "\n",
    "또한, BERT를 시작으로 BERT를 개선하는 수많은 모델들이 나오기 시작했습니다. 성능 또한 향상되었구요!\n",
    "\n",
    "따라서 modern NLP라고 하면 Transformer를 빼놓을 수 없답니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6b25f",
   "metadata": {},
   "source": [
    "## ELMO(Embedding from Language Models)\n",
    "\n",
    "ELMo는 문맥(context)을 반영한 임베딩을 pretrained model로 구현한 첫 번째 사례입니다.  \n",
    "언어 모델을 이용하여 임베딩을 한 것인데요. 이름에 포함된 LM이 바로 이 언어 모델을 가리키죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3eecd",
   "metadata": {},
   "source": [
    "### ELMo 의 구조\n",
    "\n",
    "ELMo 는 세 가지 요소로 구성되어 있습니다.\n",
    "- character-level CNN\n",
    "- bidirectional LSTM\n",
    "- ELMo 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353bf7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
