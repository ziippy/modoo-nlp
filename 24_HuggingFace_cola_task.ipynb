{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7decfeb9",
   "metadata": {},
   "source": [
    "# GLUE data의 'cola' task 를 수행하는 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce0d8b6",
   "metadata": {},
   "source": [
    "- CoLA : 문법에 맞는 문장인지 판단\n",
    "- MNLI : 두 문장의 관계 판단(entailment, contradiction, neutral)\n",
    "- MNLI-MM : 두 문장이 안 맞는지 판단\n",
    "- MRPC : 두 문장의 유사도 평가\n",
    "- SST-2 : 감정분석\n",
    "- STS-B : 두 문장의 유사도 평가\n",
    "- QQP : 두 질문의 유사도 평가\n",
    "- QNLI : 질문과 paragraph 내 한 문장이 함의 관계(entailment)인지 판단\n",
    "- RTE : 두 문장의 관계 판단(entailment, not_entailment)\n",
    "- WNLI : 원문장과 대명사로 치환한 문장 사이의 함의 관계 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cefb0f",
   "metadata": {},
   "source": [
    "`mnli` task는 이전 스텝에서 사용한 BERT를 사용하면 학습이 제대로 되지 않습니다. \n",
    "\n",
    "https://huggingface.co/models 를 참조하여 BERT가 아닌 다른 모델을 선택하세요.  \n",
    "tensorflow와 해당 모델에 대한 task로 검색하면 사용할 수 있는 모델이 나옵니다.  \n",
    "그 후 선택한 모델의 `_tokenizer_`와 해당 모델에 대한 task 와 모델 의 정보를 https://huggingface.co/transformers/index.html 에서 찾아 여러분의 프로젝트를 완성해 보세요.\n",
    "\n",
    "그냥 run_glue.py를 돌려보는 방식으로 진행하는 것을 원하는 것은 아닙니다. \n",
    "\n",
    "아래와 같은 순서를 지켜서 진행해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a12b19",
   "metadata": {},
   "source": [
    "### My\n",
    "\n",
    "CoLA task 에 대해서는 RoBERTa 를 모델로 선택  \n",
    "tokenizer 는 byte-level bpe 이용\n",
    "\n",
    "RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135def0f",
   "metadata": {},
   "source": [
    "## 라이브러리 버전을 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081dbaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "1.23.4\n",
      "4.23.1\n",
      "1.1\n",
      "2.7.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import transformers\n",
    "import argparse\n",
    "import datasets\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(transformers.__version__)\n",
    "print(argparse.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c23c4",
   "metadata": {},
   "source": [
    "## STEP 1. huggingface를 적극 활용해 CoLA 데이터셋을 분석해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173acdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/cola to C:/Users/ziipp/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1498305fb44f90b8b946ee884477d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/377k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to C:/Users/ziipp/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a93977d3e64752bb1da5c443a1a101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 8551\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1043\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1063\n",
      "    })\n",
      "})\n",
      "Counter({1: 6023, 0: 2528})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "import collections\n",
    "\n",
    "cola_dataset = load_dataset('glue', 'cola')\n",
    "print(cola_dataset)\n",
    "\n",
    "# collections을 이용해 label의 숫자를 확인할 수 있습니다.\n",
    "\n",
    "label_count = collections.Counter(cola_dataset['train']['label'])\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75ea67",
   "metadata": {},
   "source": [
    "Dataset dictionary안에 train dataset, validation dataset, test dataset으로 구성되어 있고  \n",
    "각 Dataset은 ‘sentence’, ‘label’, ‘idx’(인덱스)로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40258f9c",
   "metadata": {},
   "source": [
    "## STEP 2. Huggingface에서 제공하는 tokenizer를 활용하여 데이터셋 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5db8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b156af78528e440a97ce77eac705207d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Study\\NotebookProjects\\modoo-nlp\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ziipp\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7269e394dd5948cabd7fe1b752be8a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df38da6150a04b069b83cd75284c761a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12069cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeea379788f441e0b025f1e4ea1a9de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': [\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\", \"One more pseudo generalization or I'm giving up.\", 'The more we study verbs, the crazier they get.', 'Day by day the facts are getting murkier.'], 'label': [1, 1, 1, 1, 1], 'idx': [0, 1, 2, 3, 4]}\n",
      "{'input_ids': [[0, 2522, 964, 351, 75, 907, 42, 1966, 6, 905, 1937, 5, 220, 65, 52, 15393, 4, 2], [0, 3762, 55, 38283, 937, 1938, 8, 38, 437, 1311, 62, 4, 2], [0, 3762, 55, 38283, 937, 1938, 50, 38, 437, 1311, 62, 4, 2], [0, 133, 55, 52, 892, 47041, 6, 5, 26002, 906, 51, 120, 4, 2], [0, 10781, 30, 183, 5, 4905, 32, 562, 22802, 330, 906, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# distilbert-base-uncased 모델(distilbert 기본모델인데 대소문자를 구별하지 않는 모델)을 토크나이저로 불러오세요\n",
    "# 나는 roberta-base 이용\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def transform(data):\n",
    "  return bert_tokenizer(\n",
    "      data['sentence'],\n",
    "      truncation = True,\n",
    "      return_token_type_ids = False,\n",
    "      )\n",
    "  \n",
    "examples = cola_dataset['train'][:5]\n",
    "examples_transformed = transform(examples)\n",
    "\n",
    "print(examples)\n",
    "print(examples_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d060522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfbdf2601ba48088564125e65712010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4345dd1e984e5780aee4b7a4daf688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41145fe691749e899c2cf0387c160df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#데이터셋을 map을 이용해 토크나이징을 합니다.\n",
    "encoded_dataset = cola_dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254dd4e1",
   "metadata": {},
   "source": [
    "## STEP 3. model을 생성하여 학습 및 테스트를 진행해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c156cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bde9646aaac45b5898a0d02fe41fa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "# distilbert-base-uncased 모델(distilbert 기본모델인데 대소문자를 구별하지 않는 모델)을 pretrained model로 불러오고 label개수를 확인해 넣어주세요. [위에 있는 collections 함수를 확인하시면 됩니다]\n",
    "# 나는 roberta-bae 이용\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b102b4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziipp\\AppData\\Local\\Temp\\ipykernel_22876\\2118028897.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', 'cola')\n"
     ]
    }
   ],
   "source": [
    "#'glue/cola'  metric을 불러오세요.\n",
    "metric = load_metric('glue', 'cola')\n",
    "\n",
    "# compute_metrics를 구성해봅니다. (어렵다면 앞에 있는 노드 내용 참고하시면 됩니다)\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e0a8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "metric_name = 'loss'\n",
    "batch_size = 16\n",
    "output_dir = './data/transformers'\n",
    "\n",
    "# 다음과 같은 조건으로 training Arguments를 설정합니다.\n",
    "\"\"\"\n",
    "조건\n",
    "1. output_directory를  output_dir로 설정한다.\n",
    "2. learning_rate : 2e-5 \n",
    "3. train과 evaluation batch_size는 위에 선언하는 batch_size로 한다.\n",
    "4. train_epoch를 10으로 설정한다.\n",
    "5. weight_decay는 0.01로 설정한다.\n",
    "6. evaluation_strategy를 'steps'로 설정한다.\n",
    "7. 가장 좋은 모델을 불러온다.\n",
    "8. 가장 좋은 모델의 측정을 한다.\n",
    "\"\"\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir, # output이 저장될 경로\n",
    "    evaluation_strategy=\"steps\", #evaluation하는 빈도\n",
    "    learning_rate = 2e-5, #learning_rate\n",
    "    per_device_train_batch_size = batch_size, # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = batch_size, # evaluation 시에 batch size\n",
    "    num_train_epochs = 1, # train 시킬 총 epochs\n",
    "    weight_decay = 0.01, # weight decay\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model = metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4bb62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "D:\\Study\\NotebookProjects\\modoo-nlp\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8551\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 535\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='535' max='535' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [535/535 11:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.488200</td>\n",
       "      <td>0.488230</td>\n",
       "      <td>0.504672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./data/transformers\\checkpoint-500\n",
      "Configuration saved in ./data/transformers\\checkpoint-500\\config.json\n",
      "Model weights saved in ./data/transformers\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/transformers\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/transformers\\checkpoint-500\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./data/transformers\\checkpoint-500 (score: 0.48822999000549316).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=535, training_loss=0.47935959468378087, metrics={'train_runtime': 716.2238, 'train_samples_per_second': 11.939, 'train_steps_per_second': 0.747, 'total_flos': 90067230915480.0, 'train_loss': 0.47935959468378087, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trainer를 설정합니다.\n",
    "\"\"\"\n",
    "조건\n",
    "1. training arguments를 넣는다.\n",
    "2. automodel을 설정한다.\n",
    "3. train_dataset을 설정한다.\n",
    "4. evaluation_dataset을 validation으로 설정한다. \n",
    "5. tokenizer를 설정한다.\n",
    "6. 계산할 metrics를 설정한다.\n",
    "\"\"\"\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=model,                           # 학습시킬 model\n",
    "   args=training_arguments,                  # TrainingArguments을 통해 설정한 arguments\n",
    "   train_dataset=encoded_dataset['train'],    # training dataset\n",
    "   eval_dataset=encoded_dataset['validation'],       # evaluation dataset\n",
    "   tokenizer = bert_tokenizer,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b77581",
   "metadata": {},
   "source": [
    "현재 CoLA 데이터셋의 정확도를 측정하는 metric은 Matthews Correlations입니다.  \n",
    "https://choice-life.tistory.com/82 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1472b",
   "metadata": {},
   "source": [
    "### (보너스) CoLA processor 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb6fc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/huggingface/transformers/blob/main/src/transformers/data/processors/glue.py 해당 내용에서 찾아보세요 (raise NotImplemetedError()는 작성할때 지워주세요)\n",
    "\n",
    "from transformers.data.processors.utils import DataProcessor\n",
    "\n",
    "class ColaProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence\"].numpy().decode(\"utf-8\"),\n",
    "            None,\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        test_mode = set_type == \"test\"\n",
    "        if test_mode:\n",
    "            lines = lines[1:]\n",
    "        text_index = 1 if test_mode else 3\n",
    "        examples = []\n",
    "        for i, line in enumerate(lines):\n",
    "            guid = f\"{set_type}-{i}\"\n",
    "            text_a = line[text_index]\n",
    "            label = None if test_mode else line[1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42600221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
