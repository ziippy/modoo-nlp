{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5855384",
   "metadata": {},
   "source": [
    "## 텍스트 데이터 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99011685",
   "metadata": {},
   "source": [
    "### 노이즈 유형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4dced",
   "metadata": {},
   "source": [
    "1) 문장부호\n",
    "- 문장부호를 찾고 양쪽에 공백을 추가하는 방법\n",
    "  - 하지만 \"45,756\" 이 \"45 , 756\" 이렇게 바뀔 수 있겠지. -> 이런 경우는 불가피한 손실로 취급하고 넘어간다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fcbb39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi ,  my name is john . \n"
     ]
    }
   ],
   "source": [
    "def pad_punctuation(sentence, punc):\n",
    "    for p in punc:\n",
    "        sentence = sentence.replace(p, \" \" + p + \" \")\n",
    "\n",
    "    return sentence\n",
    "\n",
    "sentence = \"Hi, my name is john.\"\n",
    "\n",
    "print(pad_punctuation(sentence, [\".\", \"?\", \"!\", \",\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5993508",
   "metadata": {},
   "source": [
    "2) 대소문자\n",
    "- 모든 단어를 대문자나 소문자로 일괄 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fbd7854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first, open the first chapter.\n",
      "FIRST, OPEN THE FIRST CHAPTER.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"First, open the first chapter.\"\n",
    "\n",
    "print(sentence.lower())\n",
    "\n",
    "print(sentence.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59674f87",
   "metadata": {},
   "source": [
    "3) 특수문자\n",
    "- 몇 몇 특수문자 기호들을 정의해 이를 제외하곤 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551b22fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a ten year old boy.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"He is a ten-year-old boy.\"\n",
    "sentence = re.sub(\"([^a-zA-Z.,?!])\", \" \", sentence)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be74c7c",
   "metadata": {},
   "source": [
    "### 문장을 정제하는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "831625f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n",
      "but my teacher had been with me several weeks before i understood that everything has a name . \n",
      "one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n",
      "some one was drawing water and my teacher placed my hand under the spout .  \n",
      "as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n",
      "i stood still ,  my whole attention fixed upon the motions of her fingers .  \n",
      "suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n",
      "i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n",
      "that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n",
      "there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From The Project Gutenberg\n",
    "# (https://www.gutenberg.org/files/2397/2397-h/2397-h.htm)\n",
    "\n",
    "corpus = \\\n",
    "\"\"\"\n",
    "In the days that followed I learned to spell in this uncomprehending way a great many words, among them pin, hat, cup and a few verbs like sit, stand and walk. \n",
    "But my teacher had been with me several weeks before I understood that everything has a name.\n",
    "One day, we walked down the path to the well-house, attracted by the fragrance of the honeysuckle with which it was covered. \n",
    "Some one was drawing water and my teacher placed my hand under the spout. \n",
    "As the cool stream gushed over one hand she spelled into the other the word water, first slowly, then rapidly. \n",
    "I stood still, my whole attention fixed upon the motions of her fingers. \n",
    "Suddenly I felt a misty consciousness as of something forgotten—a thrill of returning thought; and somehow the mystery of language was revealed to me. \n",
    "I knew then that \"w-a-t-e-r\" meant the wonderful cool something that was flowing over my hand. \n",
    "That living word awakened my soul, gave it light, hope, joy, set it free! \n",
    "There were barriers still, it is true, but barriers that could in time be swept away.\n",
    "\"\"\" \n",
    "\n",
    "def cleaning_text(text, punc, regex):\n",
    "    # 노이즈 유형 (1) 문장부호 공백추가\n",
    "    for p in punc:\n",
    "        text = text.replace(p, \" \" + p + \" \")\n",
    "\n",
    "    # 노이즈 유형 (2), (3) 소문자화 및 특수문자 제거\n",
    "    text = re.sub(regex, \" \", text).lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "print(cleaning_text(corpus, [\".\", \",\", \"!\", \"?\"], \"([^a-zA-Z0-9.,?!\\n])\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d49009e",
   "metadata": {},
   "source": [
    "### 단어의 희소 표현과 분산 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56619c1",
   "metadata": {},
   "source": [
    "{  \n",
    "    //     [성별, 연령]  \n",
    "    남자: [-1.0, 0.0], // 이를테면 0.0 이 \"관계없음 또는 중립적\" 을 의미할 수 있겠죠!  \n",
    "    여자: [1.0, 0.0],  \n",
    "    소년: [-1.0, -0.7],  \n",
    "    소녀: [1.0, -0.7],  \n",
    "    할머니: [1.0, 0.7],  \n",
    "    할아버지: [-1.0, 0.7],  \n",
    "    아저씨: [-1.0, 0.2],  \n",
    "    아줌마: [1.0, 0.2]  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c301dc",
   "metadata": {},
   "source": [
    "{  \n",
    "    //      [성별, 연령, 과일, 색깔]  \n",
    "    남자: [-1.0, 0.0, 0.0, 0.0],  \n",
    "    여자: [1.0, 0.0, 0.0, 0.0],  \n",
    "    사과: [0.0, 0.0, 1.0, 0.5],   // 빨갛게 잘 익은 사과  \n",
    "    바나나: [0.0, 0.0, 1.0, -0.5] // 노랗게 잘 익은 바나나  \n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb6b64",
   "metadata": {},
   "source": [
    "두 고차원 벡터의 유사도는 코사인 유사도(Cosine Similarity) 를 통해 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55371d",
   "metadata": {},
   "source": [
    "우린 Embedding 레이어를 사용해 각 단어가 몇 차원의 속성을 가질지 정의하는 방식으로\n",
    "\n",
    "단어의 분산 표현(distributed representation) 를 구현하는 방식을 주로 사용하게 된다.\n",
    "\n",
    "만약 100개의 단어를 256차원의 속성으로 표현하고 싶다면 Embedding 레이어는 아래와 같이 정의된다.\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=100, output_dim=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df92cf2",
   "metadata": {},
   "source": [
    "위 단어의 분산 표현에는 우리가 일일이 정의할 수 없는 어떤 추상적인 속성들이 256차원 안에 골고루 분산되어 표현된다.\n",
    "\n",
    "희소 표현처럼 속성값을 임의로 지정해 주는 것이 아니라, 수많은 텍스트 데이터를 읽어가며 적합한 값을 찾아갈 것이다.\n",
    "\n",
    "적절히 훈련된 분산 표현 모델을 통해 우리는 단어 간의 의미 유사도를 계산하거나, 이를 feature로 삼아 복잡한 자연어처리 모델을 훈련시킬 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b52945",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8987a8",
   "metadata": {},
   "source": [
    "한 문장에서 단어의 수는 어떻게 정의할 수 있을까?\n",
    "\n",
    "\"그녀는 나와 밥을 먹는다\" 를\n",
    "- \"그녀는\" \"나와\" \"밥을\" \"먹는다\" 로 할 수 도 있고, \n",
    "- \"그녀\" \"는\" \"나\" \"와\" \"밥\" \"을\" \"먹는다\" \n",
    "로도 잘게 쪼갤 수 있다.\n",
    "\n",
    "이게, 우리가 정의할 토큰화 기법이 결정할 부분이다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103d18a",
   "metadata": {},
   "source": [
    "#### 공백 기반 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc226ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장이 포함하는 Tokens: ['in', 'the', 'days', 'that', 'followed', 'i', 'learned', 'to', 'spell', 'in', 'this', 'uncomprehending', 'way', 'a', 'great', 'many', 'words', ',', 'among', 'them', 'pin', ',', 'hat', ',', 'cup', 'and', 'a', 'few', 'verbs', 'like', 'sit', ',', 'stand', 'and', 'walk', '.', 'but', 'my', 'teacher', 'had', 'been', 'with', 'me', 'several', 'weeks', 'before', 'i', 'understood', 'that', 'everything', 'has', 'a', 'name', '.', 'one', 'day', ',', 'we', 'walked', 'down', 'the', 'path', 'to', 'the', 'well', 'house', ',', 'attracted', 'by', 'the', 'fragrance', 'of', 'the', 'honeysuckle', 'with', 'which', 'it', 'was', 'covered', '.', 'some', 'one', 'was', 'drawing', 'water', 'and', 'my', 'teacher', 'placed', 'my', 'hand', 'under', 'the', 'spout', '.', 'as', 'the', 'cool', 'stream', 'gushed', 'over', 'one', 'hand', 'she', 'spelled', 'into', 'the', 'other', 'the', 'word', 'water', ',', 'first', 'slowly', ',', 'then', 'rapidly', '.', 'i', 'stood', 'still', ',', 'my', 'whole', 'attention', 'fixed', 'upon', 'the', 'motions', 'of', 'her', 'fingers', '.', 'suddenly', 'i', 'felt', 'a', 'misty', 'consciousness', 'as', 'of', 'something', 'forgotten', 'a', 'thrill', 'of', 'returning', 'thought', 'and', 'somehow', 'the', 'mystery', 'of', 'language', 'was', 'revealed', 'to', 'me', '.', 'i', 'knew', 'then', 'that', 'w', 'a', 't', 'e', 'r', 'meant', 'the', 'wonderful', 'cool', 'something', 'that', 'was', 'flowing', 'over', 'my', 'hand', '.', 'that', 'living', 'word', 'awakened', 'my', 'soul', ',', 'gave', 'it', 'light', ',', 'hope', ',', 'joy', ',', 'set', 'it', 'free', '!', 'there', 'were', 'barriers', 'still', ',', 'it', 'is', 'true', ',', 'but', 'barriers', 'that', 'could', 'in', 'time', 'be', 'swept', 'away', '.']\n"
     ]
    }
   ],
   "source": [
    "corpus = \\\n",
    "\"\"\"\n",
    "in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n",
    "but my teacher had been with me several weeks before i understood that everything has a name . \n",
    "one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n",
    "some one was drawing water and my teacher placed my hand under the spout .  \n",
    "as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n",
    "i stood still ,  my whole attention fixed upon the motions of her fingers .  \n",
    "suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n",
    "i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n",
    "that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n",
    "there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n",
    "\"\"\"\n",
    "\n",
    "tokens = corpus.split()\n",
    "\n",
    "print(\"문장이 포함하는 Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46103cc6",
   "metadata": {},
   "source": [
    "#### 형태소 기반 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fda421",
   "metadata": {},
   "source": [
    "한국어 형태소 분석기에서 대표적인 KoNLPy를 사용해보자.  \n",
    "https://konlpy-ko.readthedocs.io/ko/v0.4.3/\n",
    "\n",
    "한국어 형태소 분석기 성능 비교  \n",
    "https://iostream.tistory.com/m/144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac588cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50078/50191814.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mKkma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mKomoran\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMecab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mOkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4c7e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4 MB 28.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.8/site-packages (from konlpy) (1.21.4)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.8/site-packages (from konlpy) (4.9.1)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
      "\u001b[K     |████████████████████████████████| 453 kB 82.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ed16c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8527a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hannanum] \n",
      "[('코로나바이러스', 'N'), ('는', 'J'), ('2019년', 'N'), ('12월', 'N'), ('중국', 'N'), ('우한', 'N'), ('에서', 'J'), ('처음', 'M'), ('발생', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('뒤', 'N'), ('전', 'N'), ('세계', 'N'), ('로', 'J'), ('확산', 'N'), ('되', 'X'), ('ㄴ', 'E'), (',', 'S'), ('새롭', 'P'), ('은', 'E'), ('유형', 'N'), ('의', 'J'), ('호흡기', 'N'), ('감염', 'N'), ('질환', 'N'), ('이', 'J'), ('ㅂ니다', 'E'), ('.', 'S')]\n",
      "[Kkma] \n",
      "[('코로나', 'NNG'), ('바', 'NNG'), ('이러', 'MAG'), ('슬', 'VV'), ('는', 'ETD'), ('2019', 'NR'), ('년', 'NNM'), ('12', 'NR'), ('월', 'NNM'), ('중국', 'NNG'), ('우', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('에', 'VV'), ('서', 'ECD'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('뒤', 'NNG'), ('전', 'NNG'), ('세계', 'NNG'), ('로', 'JKM'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETD'), (',', 'SP'), ('새', 'NNG'), ('롭', 'XSA'), ('ㄴ', 'ETD'), ('유형', 'NNG'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNG'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n",
      "[Komoran] \n",
      "[('코로나바이러스', 'NNP'), ('는', 'JX'), ('2019', 'SN'), ('년', 'NNB'), ('12월', 'NNP'), ('중국', 'NNP'), ('우', 'NNP'), ('한', 'NNP'), ('에서', 'JKB'), ('처음', 'NNG'), ('발생', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('뒤', 'NNG'), ('전', 'MM'), ('세계로', 'NNP'), ('확산', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETM'), (',', 'SP'), ('새롭', 'VA'), ('ㄴ', 'ETM'), ('유형', 'NNP'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNP'), ('질환', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n",
      "[Mecab] \n",
      "[('코로나', 'NNG'), ('바이러스', 'NNG'), ('는', 'JX'), ('2019', 'SN'), ('년', 'NNBC'), ('12', 'SN'), ('월', 'NNBC'), ('중국', 'NNP'), ('우한', 'NNG'), ('에서', 'JKB'), ('처음', 'NNG'), ('발생', 'NNG'), ('한', 'XSA+ETM'), ('뒤', 'NNG'), ('전', 'NNG'), ('세계', 'NNG'), ('로', 'JKB'), ('확산', 'NNG'), ('된', 'XSV+ETM'), (',', 'SC'), ('새로운', 'VA+ETM'), ('유형', 'NNG'), ('의', 'JKG'), ('호흡기', 'NNG'), ('감염', 'NNG'), ('질환', 'NNG'), ('입니다', 'VCP+EF'), ('.', 'SF')]\n",
      "[Okt] \n",
      "[('코로나바이러스', 'Noun'), ('는', 'Josa'), ('2019년', 'Number'), ('12월', 'Number'), ('중국', 'Noun'), ('우한', 'Noun'), ('에서', 'Josa'), ('처음', 'Noun'), ('발생', 'Noun'), ('한', 'Josa'), ('뒤', 'Noun'), ('전', 'Noun'), ('세계', 'Noun'), ('로', 'Josa'), ('확산', 'Noun'), ('된', 'Verb'), (',', 'Punctuation'), ('새로운', 'Adjective'), ('유형', 'Noun'), ('의', 'Josa'), ('호흡기', 'Noun'), ('감염', 'Noun'), ('질환', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_list = [Hannanum(),Kkma(),Komoran(),Mecab(),Okt()]\n",
    "\n",
    "kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'\n",
    "\n",
    "for tokenizer in tokenizer_list:\n",
    "    print('[{}] \\n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38dfc7-b823-4f65-9805-6347031bf4db",
   "metadata": {},
   "source": [
    "### 트러블슈팅 (mecab 설치)\n",
    "\n",
    "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh) 이걸로 설치하지만 잘 안되는 경우가 있다.\n",
    "\n",
    "그럴 땐 하나씩 하나씩 수동으로 설치한다.\n",
    "\n",
    "https://i-am-eden.tistory.com/m/9 참고\n",
    "\n",
    "모두 다 설치한 후 커널을 재시작하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b34a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mecab-ko is already installed\n",
      "mecab-ko-dic is already installed\n",
      "mecab-python is already installed\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!bash _etc/mecab.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33cc949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미\tNNP,지명,F,미,*,*,*,*\n",
      "캅\tNNP,인명,T,캅,*,*,*,*\n",
      "이\tJKS,*,F,이,*,*,*,*\n",
      "잘\tMAG,*,T,잘,*,*,*,*\n",
      "설치\tNNG,*,F,설치,*,*,*,*\n",
      "되\tXSV,*,F,되,*,*,*,*\n",
      "었\tEP,*,T,었,*,*,*,*\n",
      "는지\tEC,*,F,는지,*,*,*,*\n",
      "확인\tNNG,*,T,확인,*,*,*,*\n",
      "중\tNNB,*,T,중,*,*,*,*\n",
      "입니다\tVCP+EF,*,F,입니다,Inflect,VCP,EF,이/VCP/*+ᄇ니다/EF/*\n",
      ".\tSF,*,*,*,*,*,*,*\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "m = MeCab.Tagger()\n",
    "out= m.parse(\"미캅이 잘 설치되었는지 확인중입니다.\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9f3dd",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding(BPE)\n",
    "\n",
    "가장 많이 등장하는 바이트 쌍(Byte Pair) 을 새로운 단어로 치환하여 압축하는 작업을 반복하는 방식으로 동작\n",
    "\n",
    "만약 수많은 데이터를 사용해 만든 BPE 사전으로 모델을 학습시키고 문장을 생성하게 했다고 합시다.  \n",
    "그게 [i, am, a, b, o, y, a, n, d, you, are, a, gir, l]이라면, 어떤 기준으로 이들을 결합해서 문장을 복원하죠?  \n",
    "몽땅 한꺼번에 합쳤다간 끔찍한 일이 벌어질 것만 같습니다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29819ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaabdaaabac # 가장 많이 등장한 바이트 쌍 \"aa\"를 \"Z\"로 치환합니다.\n",
    "→ \n",
    "ZabdZabac   # \"aa\" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.\n",
    "Z=aa        # 그다음 많이 등장한 바이트 쌍 \"ab\"를 \"Y\"로 치환합니다.\n",
    "→ \n",
    "ZYdZYac     # \"ab\" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.\n",
    "Z=aa        # 여기서 작업을 멈추어도 되지만, 치환된 바이트에 대해서도 진행한다면\n",
    "Y=ab        # 가장 많이 등장한 바이트 쌍 \"ZY\"를 \"X\"로 치환합니다.\n",
    "→ \n",
    "XdXac\n",
    "Z=aa\n",
    "Y=ab\n",
    "X=ZY       # 압축이 완료되었습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9cad170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Step 1\n",
      "다음 문자 쌍을 치환: es\n",
      "변환된 Vocab:\n",
      " {'l o w ': 5, 'l o w e r ': 2, 'n e w es t ': 6, 'w i d es t ': 3} \n",
      "\n",
      ">> Step 2\n",
      "다음 문자 쌍을 치환: est\n",
      "변환된 Vocab:\n",
      " {'l o w ': 5, 'l o w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 3\n",
      "다음 문자 쌍을 치환: lo\n",
      "변환된 Vocab:\n",
      " {'lo w ': 5, 'lo w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 4\n",
      "다음 문자 쌍을 치환: low\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 5\n",
      "다음 문자 쌍을 치환: ne\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'ne w est ': 6, 'w i d est ': 3} \n",
      "\n",
      "Merge Vocab: ['es', 'est', 'lo', 'low', 'ne']\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "# 임의의 데이터에 포함된 단어들입니다.\n",
    "# 우측의 정수는 임의의 데이터에 해당 단어가 포함된 빈도수입니다.\n",
    "vocab = {\n",
    "    'l o w '      : 5,\n",
    "    'l o w e r '  : 2,\n",
    "    'n e w e s t ': 6,\n",
    "    'w i d e s t ': 3\n",
    "}\n",
    "\n",
    "num_merges = 5\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    단어 사전을 불러와\n",
    "    단어는 공백 단위로 쪼개어 문자 list를 만들고\n",
    "    빈도수와 쌍을 이루게 합니다. (symbols)\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "\n",
    "        for i in range(len(symbols) - 1):             # 모든 symbols를 확인하여 \n",
    "            pairs[symbols[i], symbols[i + 1]] += freq  # 문자 쌍의 빈도수를 저장합니다. \n",
    "        \n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "        \n",
    "    return v_out, pair[0] + pair[1]\n",
    "\n",
    "token_vocab = []\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(\">> Step {0}\".format(i + 1))\n",
    "    \n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)  # 가장 많은 빈도수를 가진 문자 쌍을 반환합니다.\n",
    "    vocab, merge_tok = merge_vocab(best, vocab)\n",
    "    print(\"다음 문자 쌍을 치환:\", merge_tok)\n",
    "    print(\"변환된 Vocab:\\n\", vocab, \"\\n\")\n",
    "    \n",
    "    token_vocab.append(merge_tok)\n",
    "    \n",
    "print(\"Merge Vocab:\", token_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22677ab6",
   "metadata": {},
   "source": [
    "### Wordpiece Model(WPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b848d5",
   "metadata": {},
   "source": [
    "이에 구글에서 BPE를 변형해 제안한 알고리즘이 바로 WPM입니다. WPM은 BPE에 대해 두 가지 차별성을 가집니다.\n",
    "- 공백 복원을 위해 단어의 시작 부분에 언더바 _ 를 추가합니다.\n",
    "- 빈도수 기반이 아닌 가능도(Likelihood)를 증가시키는 방향으로 문자 쌍을 합칩니다.\n",
    "\n",
    "즉 [_i, _am, _a, _b, o, y, _a, n, d, _you, _are, _a, _gir, l]로 토큰화를 한다는 것입니다.  \n",
    "이렇게 하면 문장을 복원하는 과정이 1) 모든 토큰을 합친 후, 2) 언더바 _를 공백으로 치환으로 마무리되어 간편하죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02255157",
   "metadata": {},
   "source": [
    "### soynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac609d",
   "metadata": {},
   "source": [
    "이외에도 한국어를 위한 토크나이저로 soynlp를 활용할 수 있습니다.  \n",
    "soynlp는 한국어 자연어 처리를 위한 라이브러리인데요.  \n",
    "토크나이저 외에도 단어 추출, 품사 판별, 전처리 기능도 제공합니다.\n",
    "\n",
    "형태소 기반의 토크나이저가 미등록 단어에 취약하기 때문에 WordPiece Model을 사용하는 것처럼,  \n",
    "형태소 기반인 koNLPy의 단점을 해결하기 위해 soynlp를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de076d03",
   "metadata": {},
   "source": [
    "### 토큰에게 의미를 부여하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb95f49",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "\n",
    "Word2Vec은 \"단어를 벡터로 만든다\"는 멋진 이름을 가지고 있습니다.  \n",
    "\n",
    "난 오늘 술을 한 잔 마셨어 라는 문장의 각 단어 즉, *동시에 등장하는 단어끼리는 연관성이 있다*는 아이디어로 시작된 알고리즘입니다.\n",
    "\n",
    "예문의 경우 다른 단어는 몰라도 술과 마셨어는 괜찮은 연관성을 캐치해낼 수 있겠네요.\n",
    "\n",
    "https://wikidocs.net/22660"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020d2f0",
   "metadata": {},
   "source": [
    "#### FastText\n",
    "\n",
    "Word2Vec은 정말 좋은 방법이지만, 연산의 빈부격차가 존재했습니다. \n",
    "\n",
    "자주 등장하지 않는 단어는 최악의 경우 단 한 번의 연산만을 거쳐 랜덤하게 초기화된 값과 크게 다르지 않은 상태로 알고리즘이 종료될 수 있습니다. FastText는 이를 해결하기 위해 BPE와 비슷한 아이디어를 적용했습니다.\n",
    "\n",
    "한국어를 위한 어휘 임베딩의 개발 -1-  \n",
    "https://brunch.co.kr/@learning/7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b20d4",
   "metadata": {},
   "source": [
    "#### ELMo - the 1st Contextualized Word Embedding\n",
    "\n",
    "위에 소개했던 Word Embedding 알고리즘들은 (역시나) 정말 훌륭하지만, 여전히 고질적인 문제점이 있습니다. \n",
    "\n",
    "바로 고정적이라는 겁니다! 무슨 말이냐면, 동음이의어를 처리할 수 없다는 얘기입니다.\n",
    "\n",
    "- 이렇게나 탐스럽고 먹음직스러운 사과를 보셨나요?\n",
    "- 저의 간절한 사과를 받아주시기 바랍니다.\n",
    "\n",
    "우리는 이 두 문장에 나오는 '사과'의 의미가 다르다는 것을 알고 있습니다.  \n",
    "그러나 Word2Vec이든 FastText이든 간에 이 두 문장에 나오는 사과의 워드 벡터값은 동일할 수밖에 없습니다.\n",
    "\n",
    "Context-sensitive Grammar를 따르는 자연어를 이해하려면 문맥(context)의 활용이 필수적입니다.\n",
    "- 여기서 '사과'의 context가 되는 것은 무엇일까요? \n",
    "  - 첫 문장이라면 탐스럽고 먹음직스러운 이 될 것이고 다음 문장이라면 간절한 이 될 것입니다. \n",
    "- 즉, 단어의 의미 벡터를 구하기 위해서는 그 단어만 필요한 것이 아니라 그 단어가 놓인 주변 단어 배치의 맥락이 함께 고려되는 Word Embedding이 필요한 것입니다. \n",
    "\n",
    "이런 개념을 Contextualized Word Embedding이라고 합니다.\n",
    "\n",
    "전이 학습 기반 NLP (1): ELMo  \n",
    "https://brunch.co.kr/@learning/12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ad017",
   "metadata": {},
   "source": [
    "## 마무리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a8f23",
   "metadata": {},
   "source": [
    "이번에 배운 내용은 토큰화와 분산 표현이 중심입니다. \n",
    "\n",
    "문장이 입력되면 적절히 토큰화를 하고 토큰을 임베딩(Embedding)을 통해 분산 표현으로 만드는 것이지요.  \n",
    "분산 표현은 벡터이므로 이제 인공지능에 활용할 수 있습니다.\n",
    "\n",
    "토큰화에 사용되는 방법은 언어마다 다른데요. 문장 구성 성분이 다르기 때문입니다.  \n",
    "조사가 있는 한국어는 형태소 기반인 koNLPy를 주로 쓰고, WordPiece Model인 SentencePiece를 쓸 수도 있어요. 물론 그 외에 다른 방법도 있습니다.\n",
    "\n",
    "토큰화를 마친 후 임베딩을 할 때는 토큰마다 독립적으로 만들지 않고 토큰 간의 관계성을 주입합니다.  \n",
    "그래야 문장을 구성할 때 적절히 사용될 수 있기 때문이에요.  \n",
    "이렇게 토큰 간의 관계성을 고려해서 만든 것으로는 Word2Vec, FastText 등이 있어요. 거기다가 문장의 문맥까지 고려하는 ELMo까지 등장했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d6471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd019d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
