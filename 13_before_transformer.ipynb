{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfaf611c",
   "metadata": {},
   "source": [
    "이번 코스에서는 자연어 처리의 꽃이라 불리는 트랜스포머(Transformer) 에 대해 알아볼 겁니다! \n",
    "\n",
    "- 트랜스포머 이전의 Attention 기법에 대해 간단하게 복습한다.\n",
    "- 트랜스포머에 포함된 모듈들을 심층적으로 이해한다.\n",
    "- 트랜스포머를 발전시키기 위해 적용된 여러 테크닉들을 알아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424368ea",
   "metadata": {},
   "source": [
    "## Attention 의 역사\n",
    "\n",
    "Sequence-to-sequence(Seq2seq)는 신경망 언어 모델, 특히 기계번역에서 혁신적인 발전을 이룩해냈습니다. \n",
    "\n",
    "기존의 단일 RNN은 번역에 적합한 구조가 아니었으나 두 개의 RNN을 결합한 Encoder-Decoder 구조를 만들어 문맥 전체를 반영한 번역이 가능하게 했죠.\n",
    "\n",
    "하지만 고정된 크기의 컨텍스트 벡터를 사용하는 것은 필연적으로 정보의 손실을 야기합니다. \n",
    "\n",
    "그래서 모든 단어를 같은 비중으로 압축하지 말고, 번역하는 데에 중요한 단어만 큰 비중을 줘서 성능을 높여보자며 Dzmitry Bahdanau가 Attention을 제안합니다.\n",
    "\n",
    "하지만 Bahdanau Attention의 문제라면 T 스텝에서 Decoder의 Hidden State를 구하기 위해 T-1 스텝의 Hidden State를 사용해야 한다는 것이었죠. \n",
    "\n",
    "이는 재귀적으로 동작하는 RNN에 역행하는 연산이므로 효율적이지 못했습니다. 이를 개선하고자 한 것이 Luong이 제안한 Attention 기법입니다!\n",
    "\n",
    "그리고 2017년, 이 흐름을 종결하기라도 하려는 듯 Attention Is All You Need 라는 충격적인 제목의 논문이 등장합니다.  \n",
    "https://arxiv.org/pdf/1706.03762.pdf 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
